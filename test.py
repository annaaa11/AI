# –î–æ–±–∞–≤—Ç–µ –≤ —Å—Ç–≤–æ—Ä–µ–Ω—É –±–∞–∑—É –¥–∞–Ω–∏—Ö —Ñ–∞–π–ª
# data/lesson_rag/huge_file.txt –ø—Ä–æ —É–º–æ–≤–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞–Ω–Ω—è –≥—É–≥–ª–æ–º
# –û—Å–∫—ñ–ª—å–∫–∏ —Ñ–∞–π–ª –Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∏–π, —Ç–æ –π–æ–≥–æ —Ç—Ä–µ–±–∞ –¥–æ–±–∞–≤–ª—è—Ç–∏
# —á–∞—Å—Ç–∏–Ω–∞–º–∏. –î–ª—è —Ü—å–æ–≥–æ:
# ÔÇ∑ –ø—Ä–æ—á–∏—Ç–∞–π—Ç–µ –≤–º—ñ—Å—Ç —Ñ–∞–π–ª—É
# ÔÇ∑ —Ä–æ–∑–¥—ñ–ª—ñ—Ç—å –π–æ–≥–æ –Ω–∞ –æ–∫—Ä–µ–º—ñ –±–ª–æ–∫–∏(–º—ñ–∂ –±–ª–æ–∫–∞–º–∏ –¥–≤–∞
# –ø–æ—Ä–æ–∂–Ω—ñ—Ö —Ä—è–¥–∫–∞, –¥–∏–≤–∏—Å—å —Ñ–∞–π–ª)
# ÔÇ∑ –æ—Ç—Ä–∏–º–∞–π—Ç–µ –ø–µ—Ä—à–∏–π —Ä—è–¥–æ–∫ –∫–æ–∂–Ω–æ–≥–æ –±–ª–æ–∫—É ‚Äì —Ü–µ –π–æ–≥–æ
# –Ω–∞–∑–≤–∞
# ÔÇ∑ —Å—Ç–≤–æ—Ä—ñ—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –±–ª–æ–∫—É. –í –º–µ—Ç–∞–¥–∞–Ω–∏—Ö:
# o –Ω–∞–∑–≤–∞ —Ñ–∞–π–ª—É
# o –Ω–∞–∑–≤–∞ –±–ª–æ–∫—É
# ÔÇ∑ —Å—Ç–≤–æ—Ä—ñ—Ç—å ID —Ç–∞ –¥–æ–±–∞–≤—Ç–µ –≤—Å–µ –≤ —ñ—Å–Ω—É—é—á—É –±–∞–∑—É –¥–∞–Ω–∏—Ö
# ÔÇ∑ –¥–æ–±–∞–≤—Ç–µ ID —É json —Ñ–∞–π–ª
# ÔÇ∑ –ø–µ—Ä–µ–≤—ñ—Ä—Ç–µ –∞–≥–µ–Ω—Ç–∞

from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from pinecone import Pinecone, ServerlessSpec
from langchain_core.documents import Document
from langchain_pinecone import PineconeVectorStore
from langchain_community.utilities import GoogleSerperAPIWrapper
from langgraph.prebuilt import create_react_agent
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

import os
import dotenv
import json
from uuid import uuid4

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–ª—é—á—ñ–≤
dotenv.load_dotenv()
api_key = os.getenv('GEMINI_API_KEY')
pinecone_api_key = os.getenv('PINECONE_API_KEY')

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/text-embedding-004",
    google_api_key=api_key
)

llm = ChatGoogleGenerativeAI(
    model='gemini-2.0-flash',
    google_api_key=api_key
)

# –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Pinecone
pc = Pinecone(api_key=pinecone_api_key)
index_name = "task1"

if not pc.has_index(index_name):
    pc.create_index(
        name=index_name,
        dimension=768,
        metric="cosine",
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )

index = pc.Index(index_name)
vector_store = PineconeVectorStore(index=index, embedding=embeddings)

# –î–æ–¥–∞—Ç–∏ –≤–µ–ª–∏–∫–∏–π —Ñ–∞–π–ª —á–∞—Å—Ç–∏–Ω–∞–º–∏
huge_file_path = "data/lesson_rag/huge_file.txt"

if os.path.exists(huge_file_path):
    with open(huge_file_path, "r", encoding="utf-8") as f:
        huge_text = f.read()

    raw_blocks = huge_text.split("\n\n\n")
    blocks = []
    for block in raw_blocks:
        block = block.strip()
        if block != "":
            blocks.append(block)

    docs = []
    doc_ids = []
    id_data = {}

    for block in blocks:
        lines = block.strip().splitlines()
        if not lines:
            continue
        block_title = lines[0].strip()
        content = "\n".join(lines).strip()
        doc = Document(
            page_content=content,
            metadata={
                "file_path": huge_file_path,
                "block_title": block_title
            }
        )
        docs.append(doc)
        new_id = str(uuid4())
        doc_ids.append(new_id)
        file_name = os.path.basename(huge_file_path)  # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ –ø—É—Ç–∏
        key_name = file_name + " | " + block_title  # –°–∫–ª–µ–∏–≤–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –±–ª–æ–∫–∞
        id_data[key_name] = new_id  # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å ID –ø–æ –∫–ª—é—á—É

    # –î–æ–¥–∞—Ç–∏ –¥–æ —ñ—Å–Ω—É—é—á–æ–≥–æ JSON
    json_path = "data_ai.json"
    if os.path.exists(json_path):
        with open(json_path, "r", encoding="utf-8") as jf:
            existing_data = json.load(jf)
    else:
        existing_data = {}

    existing_data.update(id_data)

    with open(json_path, "w", encoding="utf-8") as jf:
        json.dump(existing_data, jf, indent=2, ensure_ascii=False)

    # –î–æ–¥–∞—Ç–∏ —É –≤–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É
    vector_store.add_documents(docs, ids=doc_ids)
    print(f"–î–æ–¥–∞–Ω–æ {len(docs)} –±–ª–æ–∫—ñ–≤ –∑ huge_file.txt –¥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏.")
else:
    print("–§–∞–π–ª huge_file.txt –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –¥–æ–¥–∞–≤–∞–Ω–Ω—è.")

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–æ—à—É–∫—É –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
def doc_ser(user_text: str):
    """
    –®—É–∫–∞—î –¥–æ–∫—É–º–µ–Ω—Ç–∏, —è–∫—ñ –ø—ñ–¥—Ö–æ–¥—è—Ç—å –ø—ñ–¥ –∑–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.
    –¶–µ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –ø—Ä–æ —à—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç.
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤, –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –ø–æ—à—É–∫—É
    """
    print("doc_ser")
    docs = vector_store.similarity_search(user_text, k=2)
    return docs

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–≥–µ–Ω—Ç–∞
agent = create_react_agent(
    model=llm,
    tools=[doc_ser]
)

# –ü–æ—á–∞—Ç–∫–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–∏
messages = {"messages": [
    SystemMessage("""
    –¢–∏ —á–∞—Ç-–±–æ—Ç, —è–∫–∏–π –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è –ø—Ä–æ —à—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç.
    –°–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–±—É–π –∑–Ω–∞–π—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —É –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —á–µ—Ä–µ–∑ "doc_ser".
    –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π–¥–µ—à ‚Äî –¥–∞–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å —Å–∞–º.
    """)
]}

# –ß–∞—Ç-—Ü–∏–∫–ª
while True:
    user_input = input("–í–∏: ")
    if user_input.strip() == '':
        print("üëã –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è —á–∞—Ç—É.")
        break

    user_message = HumanMessage(user_input)
    messages["messages"].append(user_message)

    messages = agent.invoke(messages)

    ai_message = messages["messages"][-1]
    print("–ë–æ—Ç:", ai_message.content)
